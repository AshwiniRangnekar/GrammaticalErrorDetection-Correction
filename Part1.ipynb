{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZgGNXZEfywz"
   },
   "source": [
    "#Task 1: Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "deLPHwbIX3-r"
   },
   "outputs": [],
   "source": [
    "# Download gutenberg dataset\n",
    "# This is the dataset on which all your models will be trained\n",
    "# https://drive.google.com/file/d/0B2Mzhc7popBga2RkcWZNcjlRTGM/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization into sentences and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08q0ospof2z9"
   },
   "outputs": [],
   "source": [
    "import sys, re\n",
    "import math\n",
    "import string\n",
    "\n",
    "'''\n",
    "This function removes the punctuation marks, special symbols from the tokens\n",
    "'''\n",
    "def remove_punctuations(text):\n",
    "    punct = [\"[\",\"]\",\"'s\",\"(\",\")\",\"!\",\"\\\"\",\"%\",\"$\",\"*\",\"&\",\"^\",\"=\",\"+\",\"`\",\"'\",\",\"]    \n",
    "    for x in punct:\n",
    "        text = text.replace(x,\"\")\n",
    "    \n",
    "    for x in text:\n",
    "        if x.isdigit():\n",
    "            text = text.replace(x,\"\")\n",
    "        \n",
    "    text = text.replace(\"\\n\\n\" , \". \")\n",
    "    text = text.replace(\".\\n\\n\" , \". \")\n",
    "    text = text.replace(\"..\" , \".\")\n",
    "    \n",
    "    p = [\":--\",\"-\",\"\\n\",\"  \",\"_\",\",\",\";\",\":\",\"?\",\"/\",\"{\",\"}\"]    \n",
    "    for pp in p:\n",
    "        text = text.replace(pp , \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function tokenizes the corpus into words and sentences\n",
    "'''\n",
    "def tokenize(text,sent,tok):\n",
    "    # Given text, tokenise into sentences and words. Take into consideration various delimiters.\n",
    "    text = text.lower()\n",
    "    text = remove_punctuations(text)\n",
    "    \n",
    "    sentences = []\n",
    "    s = \"<SOS> \"\n",
    "    for word in text:\n",
    "        if word.endswith('.'):\n",
    "            word = word[:-1]\n",
    "            s = s + word+\" <EOS>\"\n",
    "         \n",
    "            if s != \"<SOS>  <EOS>\":\n",
    "                s = s.replace(\"  \",\" \")\n",
    "                sentences.append(s.strip())\n",
    "            s = \"<SOS> \"\n",
    "        \n",
    "        else:\n",
    "            s = s + word\n",
    "            \n",
    "    for sent1 in sentences:\n",
    "        if sent1 == \" \":\n",
    "            remove(sent1)\n",
    "            \n",
    "    sent.extend(sentences)        \n",
    "    tokens = text.split()\n",
    "    lower_tokens = []\n",
    "    for x in tokens:\n",
    "        lower_tokens.append(x.lower())\n",
    "    punct = string.punctuation\n",
    "    table = str.maketrans('','',punct)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    tok.extend(stripped)\n",
    "    return tok,sent\n",
    "\n",
    "'''\n",
    "Reading the corpus (File read operation) and calling moving further to clean the corpus and tokenize it.\n",
    "'''\n",
    "import glob\n",
    "def get_tokens():\n",
    "    sentences = []\n",
    "    tokens = []\n",
    "    path = '/home/ashwini/Dropbox/Sem3/NLP/P1A/Gutenberg/txt/*.txt'\n",
    "    \n",
    "    files = glob.glob(path)\n",
    "\n",
    "    for name in files:\n",
    "        file = open(name, 'rt')\n",
    "        text = file.read()\n",
    "        tokens,sentences = tokenize(text,sentences,tokens)\n",
    "        \n",
    "    return tokens,sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7gwBEe5bgAbn"
   },
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigramLangModel(words):\n",
    "    global corpus_length\n",
    "    \n",
    "    for word in words:\n",
    "        if word != start_sentence and word != end_sentence:\n",
    "            freq_dict_unigram[word] = freq_dict_unigram.get(word,0) + 1\n",
    "            corpus_length += 1\n",
    "        \n",
    "def bigramLangModel(a,sentences):\n",
    "    unigramLangModel(a)\n",
    "    unique_bigrams = set()\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.split()\n",
    "        \n",
    "        previous_word = None\n",
    "        for word in sentence:           \n",
    "            if previous_word != None:\n",
    "                freq_dict_bigram[(previous_word,word)] = freq_dict_bigram.get((previous_word,word),0) + 1\n",
    "\n",
    "            if previous_word != start_sentence and previous_word != end_sentence:\n",
    "                unique_bigrams.add((previous_word,word))\n",
    "            previous_word = word\n",
    "    \n",
    "def trigramLangModel(a,sentences):\n",
    "    bigramLangModel(a,sentences)\n",
    "    unique_trigrams = set()\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.split()\n",
    "        prev1_word = None\n",
    "        prev2_word = None\n",
    "        for word in sentence:\n",
    "            if prev1_word != None and prev2_word != None:\n",
    "                freq_dict_trigram[(prev1_word,prev2_word,word)] = freq_dict_trigram.get((prev1_word,prev2_word,word),0) + 1\n",
    "\n",
    "                if prev1_word != start_sentence and prev1_word != end_sentence:\n",
    "                    unique_trigrams.add((prev1_word,prev2_word,word))\n",
    "            prev1_word = prev2_word\n",
    "            prev2_word = word\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq_classes(ugrams):\n",
    "    eq = {}\n",
    "    for k, v in ugrams.items():\n",
    "        if v not in eq:\n",
    "            eq[v] = []\n",
    "        eq[v].append(k)\n",
    "    return eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Th9ogrogKTO"
   },
   "outputs": [],
   "source": [
    "def compare_helper(l1,l2):                                           \n",
    "    i=0\n",
    "    j=0\n",
    "    while i<len(l1) and j<len(l2):\n",
    "        if(l1[i]!=l2[j]):\n",
    "            return 1\n",
    "        i+=1\n",
    "        j+=1\n",
    "   \n",
    "    if(len(l1) != len(l2)):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def complete_ngram(ngram, n):\n",
    "# Given (N-1) gram, and the value 'N', print the possibilities that complete the n-gram\n",
    "# and plot them in decresing order of frequency\n",
    "    temp1 = {}\n",
    "    if n==2:\n",
    "        temp = ngram[-(n-1):]\n",
    "        for k,v in freq_dict_bigram.items():\n",
    "                if compare_helper(k[:-1],temp) == 0:\n",
    "                    #print(str(k[-1]) + \" | \" + str(v))\n",
    "                    temp1[k[-1]] = v\n",
    "\n",
    "    elif n>=3:\n",
    "        temp = ngram[-2:]\n",
    "        for k,v in freq_dict_trigram.items():\n",
    "                if compare_helper(k[-3:-1],temp) == 0:\n",
    "                    #print(\":\" + str(k) + \": | \" + str(v))\n",
    "                    temp1[k] = v\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print(\"\\n\\t\\t---------Suggested nth word----------\\n\\t\\t-------------------------------------\\n\")\n",
    "    sorted_list = sorted(temp1.items(),key = itemgetter(1),reverse = True)\n",
    "    for k in sorted_list:\n",
    "        print(str(k[0]) + \"   \" + str(k[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only 1 of the next 3 have to be implemented\n",
    "\n",
    "def witten_bell(n_grams):                       \n",
    "  # perform Witten-Bell smoothing\n",
    "    pass\n",
    "def kneser_ney(n_grams):\n",
    "  # perform Kneser-Ney smoothing\n",
    "    pass\n",
    "# only 1 of the next 2 have to be implemented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --------------------------------- Perform Laplace smoothing ---------------------------------------------\n",
    "def laplace_smoothing(num,deno,ngram):\n",
    "    num += 1    \n",
    "    if ngram == 2:\n",
    "        deno += len(freq_dict_unigram) + 1\n",
    "    elif ngram == 3:\n",
    "        deno += len(freq_dict_bigram) + 1\n",
    "    return num,deno\n",
    "\n",
    "# ----------------------------- Perform Good Turing smoothing ---------------------------------------------\n",
    "def good_turing(dict1):  \n",
    "    \n",
    "    Nr = eq_classes(dict1)\n",
    "    nr_counts = {k : len(v) for k, v in Nr.items()}\n",
    "    nr_probs = {k : (k*v)/float(N) for k, v in nr_counts.items()}\n",
    "    sorted_nrs = sorted(nr_counts.items())\n",
    "    sorted_probs = sorted(nr_probs.items())\n",
    "    MAX = sorted_nrs[0][1]\n",
    "    \n",
    "    for r, nr in Nr.items():\n",
    "        if (r+1) in Nr:\n",
    "            return ((r+1) * len(Nr[r+1])) / float(N) \n",
    "        else:\n",
    "            return MAX*r**-2 / float(N)\n",
    "    \n",
    "# --------------------------------------------------- Backoff ---------------------------------------------\n",
    "\n",
    "def backoff(word,sent):\n",
    "    score = 0.0\n",
    "    \n",
    "    for i in range(len(sent)):\n",
    "        if sent[i]!='<SOS>' and sent[i]!='<EOS>' and sent[i-1]!='<SOS>': \n",
    "            bi_key = (sent[i - 1],sent[i])\n",
    "            count_bigram = freq_dict_bigram.get(bi_key,0)\n",
    "            #count_unigram = freq_dict_unigram[sent[i - 1]]\n",
    "            count_unigram = freq_dict_unigram.get(sent[i - 1],0)\n",
    "            if count_bigram > 0:\n",
    "                score += math.log(count_bigram)\n",
    "                score -= math.log(count_unigram)\n",
    "            else:\n",
    "                count_unigram = freq_dict_unigram[sent[i]]\n",
    "                score += math.log(count_unigram + 1)\n",
    "                score -= math.log(corpus_length * 2)\n",
    "                score += math.log(0.4)\n",
    "    print(type(score)) \n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IjoF5Wlsaf_M"
   },
   "source": [
    "#Task 2: Unigrams and Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oehobp6dbXp7"
   },
   "outputs": [],
   "source": [
    "def Print_top10_bottom10(ngrams):\n",
    "    temp =  {}\n",
    "    for key,value in ngrams.items():\n",
    "        temp_str = str(key)\n",
    "        temp[temp_str] = value\n",
    "    \n",
    "    # Calculate unigram frequencies and plot them in the descending order of frequency\n",
    "    temp = OrderedDict(sorted(temp.items(),key = itemgetter(1),reverse = True))       \n",
    "    word = list(temp.keys())\n",
    "    freq = list(temp.values())\n",
    "#     plot_ngram_freqs(word,freq)\n",
    "    \n",
    "    # Print top 10 \n",
    "    w = []\n",
    "    print(\"\\n Top 10 \\n---------\\n\")\n",
    "    w = sorted(temp.items(),key = itemgetter(1),reverse = True)\n",
    "    for i in w[:10]:\n",
    "        print(str(i[0]) + \"  |  \" + str(temp[i[0]]))\n",
    "    \n",
    "    print(\"\\n Bottom 10 \\n----------\\n\")\n",
    "    for i in w[len(temp)-10:]:\n",
    "        print(str(i[0]) + \"  |  \" + str(temp[i[0]]))\n",
    "    \n",
    "def plot_ngram_freqs(word,freq):\n",
    "\n",
    "    import plotly.plotly as py\n",
    "    import plotly.graph_objs as go\n",
    "    \n",
    "    # Create and style traces\n",
    "    trace0 = go.Scatter(x = word,y = freq,name = 'Plot Ngrams_freq',line = dict(color = ('rgb(205, 12, 24)'),width = 2))   \n",
    "    data = [trace0]\n",
    "\n",
    "    # Edit the layout\n",
    "    layout = dict(title = 'Plot of ngrams vs frequency',xaxis = dict(title = 'word'),yaxis = dict(title = 'frequency'),)\n",
    "\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    py.iplot(fig, filename='styled-line')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- Spelling checker ------------------------------------------\n",
    "\n",
    "alphabet = set('abcdefghijklmnopqrstuvwxyz')\n",
    "\n",
    "def probabilistic_spellcorrection(word):\n",
    "#     print(a)\n",
    "    char_unigram = {}\n",
    "    for cu in a:\n",
    "        for l in cu:\n",
    "            char_unigram[l] = char_unigram.get(l,0) + freq_dict_unigram[cu]\n",
    "        \n",
    "    char_bigram = {}\n",
    "    for cb in a:\n",
    "        for index in range(0,len(cb)-1):\n",
    "            char_bigram[(cb[index],cb[index+1])] = char_bigram.get((cb[index],cb[index+1]),0) + freq_dict_unigram[cb]\n",
    "            \n",
    "    print(char_unigram)\n",
    "    print(\"\\n\\n\")\n",
    "    print(char_bigram)\n",
    "\n",
    "def edit_distance(word):\n",
    "    splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes    = [a + b[1:] for a, b in splits if b]\n",
    "    transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "    replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "    inserts    = [a + c + b for a, b in splits for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def spell_checker(word, n_grams):\n",
    "# Given a word check if the spelling is correct using your Language model\n",
    "    word = word.lower()\n",
    "    return (set(edit_distance(word)) & set(freq_dict_unigram.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgPpfeyYgLUv"
   },
   "source": [
    "#Task 3 : Grammaticality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build your language model - This function builds the Language Model.\n",
    "It calls trigramLangModel which inside calls BigramLangModel and which again calls UnigramLangModel thus all three\n",
    "models are created one by one using the tokens.\n",
    "'''\n",
    "def language_model(tokens,sentences):\n",
    "    global freq_dict_unigram\n",
    "    global freq_dict_bigram    \n",
    "    trigramLangModel(tokens,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xX0RvOpLgYgR"
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Here if smoothing is set True then the following smoothing techniques can be used depending on choice\n",
    "1. Laplace Smoothing\n",
    "2. Good Turing\n",
    "3. Backoff\n",
    "'''\n",
    "def unigram_sentence_probability(sent):            # To calculate the score for a sentence using unigram Language model\n",
    "    unigrams = []\n",
    "    un = []\n",
    "    unigrams,un = tokenize(sent,unigrams,un)\n",
    "    sum = 0.0\n",
    "\n",
    "    for uni in unigrams:\n",
    "        pro = cal_unigram_probability(uni,True,1,sent)\n",
    "        if pro!=0:\n",
    "            sum += pro\n",
    "    return sum\n",
    "\n",
    "def bigram_sentence_probability(sent):              # To calculate the score for a sentence using bigram Language model\n",
    "    sum = 0.0\n",
    "    previous_word = None\n",
    "    sent = sent.split()\n",
    "    for word in sent:\n",
    "        if previous_word != None:\n",
    "            bigram_prob = cal_bigram_probability(previous_word,word,True,1,sent)\n",
    "            if bigram_prob!=0:\n",
    "                sum += bigram_prob\n",
    "        previous_word = word\n",
    "    return sum\n",
    "\n",
    "def trigram_sentence_probability(sent):            # To calculate the score for a sentence using trigram Language model\n",
    "    sum = 0.0\n",
    "    unique_trigrams = set()\n",
    "    sent = sent.split()\n",
    "    prev1_word = None\n",
    "    prev2_word = None\n",
    "    for word in sent:\n",
    "        if prev1_word != None and prev2_word != None:\n",
    "            trigram_prob = cal_trigram_probability(prev1_word,prev2_word,word,True,1,sent)\n",
    "            if trigram_prob!=0:\n",
    "                sum += trigram_prob\n",
    "        prev1_word = prev2_word\n",
    "        prev2_word = word\n",
    "    return sum    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Here if smoothing is set True then the following smoothing techniques can be used depending on choice\n",
    "1. Laplace Smoothing\n",
    "2. Good Turing\n",
    "3. Backoff\n",
    "This function is the utility function which actually calculates the score for the sentences according to the choice of\n",
    "Language Model and Smoothing Technique.\n",
    "'''\n",
    "\n",
    "def cal_unigram_probability(word,smoothing,sm,sent):                       # Using smoothing for unigram Language model\n",
    "    num = freq_dict_unigram.get(word,0)\n",
    "    denum = corpus_length\n",
    "    \n",
    "    if smoothing:\n",
    "        if sm == 1:\n",
    "            num,denum = laplace_smoothing(num,denum,2)\n",
    "        elif sm == 2:\n",
    "            return good_turing(freq_dict_unigram)\n",
    "        elif sm == 3:\n",
    "            return backoff(word,sent)\n",
    "    if num == 0 or denum == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(num) / float(denum)\n",
    "    \n",
    "def cal_bigram_probability(prev_word,word,smoothing,sm,sent):              # Using smoothing for bigram Language model\n",
    "    num = freq_dict_bigram.get((prev_word,word),0)\n",
    "    denum = freq_dict_unigram.get(prev_word,0)\n",
    "    \n",
    "    if smoothing:\n",
    "        if sm == 1:\n",
    "            num,denum = laplace_smoothing(num,denum,2)\n",
    "        elif sm == 2:\n",
    "            return good_turing(freq_dict_unigram)\n",
    "        elif sm == 3:\n",
    "            return backoff(word,sent)\n",
    "    if num == 0 or denum == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(num)/float(denum)\n",
    "    \n",
    "def cal_trigram_probability(prev1_word,prev2_word,word,smoothing,sm,sent): # Using smoothing for trigram Language model\n",
    "    num = freq_dict_trigram.get((prev1_word,prev2_word,word),0)\n",
    "    denum = freq_dict_bigram.get((prev1_word,prev2_word),0)\n",
    "    \n",
    "    if smoothing:\n",
    "        if sm == 1:                                                    \n",
    "            num,denum = laplace_smoothing(num,denum,3)\n",
    "        elif sm == 2:\n",
    "            return good_turing(freq_dict_bigram)\n",
    "        elif sm == 3:\n",
    "            return backoff(word,sent)        \n",
    "    if num == 0 or denum == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(num)/float(denum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "# Global variables and data structures are declared here which will be used throughout the program.\n",
    "start_sentence = \"<SOS>\"                                    # Denotes the start of sentence\n",
    "end_sentence = \"<EOS>\"                                      # Denotes the end of sentence\n",
    "freq_dict_unigram = {}                                      # A dictionary maintained for unigrams and their frequency\n",
    "freq_dict_bigram = {}                                       # A dictionary maintained for bigrams and their frequency\n",
    "freq_dict_trigram = {}                                      # A dictionary maintained for trigrams and their frequency\n",
    "corpus_length = 0                                           # It tells the number of different words in corpus\n",
    "temp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "To tokenize the corpus into words and sentences get_tokens function is used which returns \n",
    "1. the words in list a, and \n",
    "2. the sentences in list b\n",
    "'''\n",
    "\n",
    "a,b = get_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes the tokens as inputs and build Language Models using those tokens.\n",
    "The Language Models used in this program are:-\n",
    "1. Unigram Lang Model\n",
    "2. Bigram Lang Model\n",
    "3. Trigram Lang Model\n",
    "'''\n",
    "language_model(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top 10 \n",
      "---------\n",
      "\n",
      "the  |  64949\n",
      "of  |  37833\n",
      "to  |  32303\n",
      "and  |  28508\n",
      "a  |  19388\n",
      "in  |  19382\n",
      "that  |  17166\n",
      "i  |  16098\n",
      "it  |  14746\n",
      "is  |  10591\n",
      "\n",
      " Bottom 10 \n",
      "----------\n",
      "\n",
      "runt  |  1\n",
      "absurdities  |  1\n",
      "fluted  |  1\n",
      "luis  |  1\n",
      "honeycombed  |  1\n",
      "editorialized  |  1\n",
      "undisciplined  |  1\n",
      "soilers  |  1\n",
      "babies  |  1\n",
      "voyons  |  1\n",
      "\n",
      " Top 10 \n",
      "---------\n",
      "\n",
      "('of', 'the')  |  10914\n",
      "('in', 'the')  |  5179\n",
      "('<SOS>', 'i')  |  4957\n",
      "('to', 'the')  |  4371\n",
      "('<SOS>', 'the')  |  3978\n",
      "('<SOS>', '<EOS>')  |  2805\n",
      "('to', 'be')  |  2457\n",
      "('it', 'is')  |  2398\n",
      "('<SOS>', 'it')  |  2244\n",
      "('<SOS>', 'a')  |  2149\n",
      "\n",
      " Bottom 10 \n",
      "----------\n",
      "\n",
      "('can', 'use')  |  1\n",
      "('rise', 'said')  |  1\n",
      "('laborer', 'i')  |  1\n",
      "('of', 'unscrupulous')  |  1\n",
      "('henderson', 'are')  |  1\n",
      "('men', 'some')  |  1\n",
      "('territory', 'it')  |  1\n",
      "('eyes', 'expressed')  |  1\n",
      "('grant', 'drank')  |  1\n",
      "('interrogatively', '<EOS>')  |  1\n",
      "\n",
      " Top 10 \n",
      "---------\n",
      "\n",
      "('<SOS>', 'a', '<EOS>')  |  1409\n",
      "('<SOS>', 'lincoln', '<EOS>')  |  1278\n",
      "('the', 'united', 'states')  |  1276\n",
      "('of', 'the', 'united')  |  893\n",
      "('<SOS>', 'it', 'is')  |  736\n",
      "('<SOS>', 'telegram', 'to')  |  571\n",
      "('<SOS>', 'i', 'have')  |  497\n",
      "('<SOS>', 'it', 'was')  |  477\n",
      "('<SOS>', 'executive', 'mansion')  |  474\n",
      "('<SOS>', 'mr', '<EOS>')  |  467\n",
      "\n",
      " Bottom 10 \n",
      "----------\n",
      "\n",
      "('are', 'legion', 'have')  |  1\n",
      "('to', 'his', 'canvas')  |  1\n",
      "('summerhaye', 'lips', '<EOS>')  |  1\n",
      "('great', 'florid', 'baroque')  |  1\n",
      "('those', 'under', 'which')  |  1\n",
      "('of', 'webster', 'sometimes')  |  1\n",
      "('<SOS>', 'their', 'clothes')  |  1\n",
      "('and', 'that', 'little')  |  1\n",
      "('topes', 'nodded', '<EOS>')  |  1\n",
      "('no', 'proceedings', 'on')  |  1\n"
     ]
    }
   ],
   "source": [
    "N = sum(freq_dict_unigram.values())\n",
    "\n",
    "'''\n",
    "To Print the Top 10 and Bottom 10 frequency words for all types of Language Models, and\n",
    "Plotting frequencies vs ngrams with descending order of frequencies for all types of Language models.\n",
    "'''\n",
    "Print_top10_bottom10(freq_dict_unigram)\n",
    "Print_top10_bottom10(freq_dict_bigram)\n",
    "Print_top10_bottom10(freq_dict_trigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suggestions:  \n",
      "---------------\n",
      "{'rats', 'mate', 'hats', 'mais', 'eats', 'pats', 'mass', 'mates', 'oats', 'bats', 'mars', 'maps', 'cats', 'tats', 'nats'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "It takes a word as input and then checks for the spelling whether it is correct or not according to the vocabulary \n",
    "of corpus. If the word is not from corpus then spell_checker returns a list of words which were probably misspelled.\n",
    "'''\n",
    "suggestions = spell_checker(\"mats\",freq_dict_unigram)\n",
    "print(\"\\nSuggestions:  \\n---------------\\n\" + str(suggestions) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Unigram model \n",
      " --------------\n",
      "\n",
      "0.3997798170064511\n",
      "0.3025435163492585\n",
      "\n",
      " Bigram model \n",
      " --------------\n",
      "\n",
      "0.34558719409489447\n",
      "0.208055697168767\n",
      "\n",
      " Trigram model \n",
      " --------------\n",
      "\n",
      "0.0004802159828236821\n",
      "0.0001585872537203643\n",
      "\n",
      "\n",
      "\n",
      "\t\t---------Suggested nth word----------\n",
      "\t\t-------------------------------------\n",
      "\n",
      "('is', 'there', 'any')   20\n",
      "('is', 'there', 'in')   9\n",
      "('is', 'there', 'a')   9\n",
      "('is', 'there', 'anything')   8\n",
      "('is', 'there', 'not')   5\n",
      "('is', 'there', 'to')   4\n",
      "('is', 'there', '<EOS>')   4\n",
      "('is', 'there', 'such')   4\n",
      "('is', 'there', 'no')   3\n",
      "('is', 'there', 'may')   3\n",
      "('is', 'there', 'then')   3\n",
      "('is', 'there', 'has')   3\n",
      "('is', 'there', 'is')   2\n",
      "('is', 'there', 'they')   2\n",
      "('is', 'there', 'can')   2\n",
      "('is', 'there', 'it')   2\n",
      "('is', 'there', 'so')   1\n",
      "('is', 'there', 'anyone')   1\n",
      "('is', 'there', 'nothing')   1\n",
      "('is', 'there', 'something')   1\n",
      "('is', 'there', 'one')   1\n",
      "('is', 'there', 'we')   1\n",
      "('is', 'there', 'just')   1\n",
      "('is', 'there', 'nobody')   1\n",
      "('is', 'there', 'for')   1\n",
      "('is', 'there', 'expressed')   1\n",
      "('is', 'there', 'and')   1\n",
      "('is', 'there', 'but')   1\n",
      "('is', 'there', 'given')   1\n",
      "('is', 'there', 'now')   1\n",
      "('is', 'there', 'room')   1\n",
      "('is', 'there', 'reason')   1\n",
      "('is', 'there', 'seriously')   1\n",
      "('is', 'there', 'or')   1\n",
      "('is', 'there', 'expounded')   1\n",
      "('is', 'there', 'going')   1\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences in which one is correct according to grammar but the second one is grammatically not correct\n",
    "'''\n",
    "GRAMMATICALITY TEST:\n",
    "\n",
    "Here different Language models with different smoothing techniques are used which takes the sentences and returns a score\n",
    "telling the sentence's correctness.\n",
    "More is the score More correct is the sentence according to the corpus grammar.\n",
    "Lesser is the score it means the sentence is not according to corpus grammar.\n",
    "'''\n",
    "\n",
    "text = \"<SOS> It will be observed that the philosophical admonitions in the letter to his brother, Johnston, were written on the same sheet with the letter to his father <EOS>\"\n",
    "text1 = \"<SOS> was philosophical his brother admonitions in the letter to, Johnston with the, on the some sheet were written letter to his father <EOS>\"\n",
    "\n",
    "print(\"\\n Unigram model \\n --------------\\n\")\n",
    "print(unigram_sentence_probability(text))\n",
    "print(unigram_sentence_probability(text1))\n",
    "\n",
    "print(\"\\n Bigram model \\n --------------\\n\")\n",
    "print(bigram_sentence_probability(text))\n",
    "print(bigram_sentence_probability(text1))\n",
    "\n",
    "print(\"\\n Trigram model \\n --------------\\n\")\n",
    "print(trigram_sentence_probability(text))\n",
    "print(trigram_sentence_probability(text1))\n",
    "\n",
    "'''\n",
    "Here if input is given as (N-1) grams then it predicts Nth gram which may come after it according to corpus.\n",
    "First Parameter may be unigram,bigram,triagram \n",
    "Second Parameter tells how many grams to be considered for predicting the next nth gram.\n",
    "'''\n",
    "print(\"\\n\")\n",
    "complete_ngram([\"is\",\"there\"],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Error Correction Demo.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
